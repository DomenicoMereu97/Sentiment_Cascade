{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xrdtQz-2ydF"
   },
   "source": [
    "# Data Science Lab Project 2022 Winter Session (TF-IDF part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IHtMrNqt_azi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-20 10:57:25--  https://dbdmg.polito.it/dbdmg_web/wp-content/uploads/2021/12/DSL2122_january_dataset.zip\n",
      "Risoluzione di dbdmg.polito.it (dbdmg.polito.it)... 130.192.163.163\n",
      "Connessione a dbdmg.polito.it (dbdmg.polito.it)|130.192.163.163|:443... connesso.\n",
      "Richiesta HTTP inviata, in attesa di risposta... 200 OK\n",
      "Lunghezza: 18640208 (18M) [application/zip]\n",
      "Salvataggio in: «DSL2122_january_dataset.zip.1»\n",
      "\n",
      "DSL2122_january_dat 100%[===================>]  17,78M  6,36MB/s    in 2,8s    \n",
      "\n",
      "2022-01-20 10:57:28 (6,36 MB/s) - «DSL2122_january_dataset.zip.1» salvato [18640208/18640208]\n",
      "\n",
      "Archive:  DSL2122_january_dataset.zip\n",
      "checkdir:  cannot create extraction directory: /content/data\n",
      "           No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://dbdmg.polito.it/dbdmg_web/wp-content/uploads/2021/12/DSL2122_january_dataset.zip\" \n",
    "!unzip 'DSL2122_january_dataset.zip' -d '/content/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Qf2E83h0_qGj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import string as st\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fLwfWwZg_5YR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/giovannimantegna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "u7XT6ZmeBm0U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emot in /Users/giovannimantegna/opt/anaconda3/lib/python3.8/site-packages (3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emot --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AroBneyA_6p0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/giovannimantegna/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jGc6Ct_N_hKt",
    "outputId": "4bfc66c2-2bdb-40cb-d5d9-9be12b1b5ebe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovannimantegna/opt/anaconda3/lib/python3.8/site-packages/dateutil/parser/_parser.py:1213: UnknownTimezoneWarning: tzname PDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"development.csv\", parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NK9_ztgY8VhY",
    "outputId": "31bf41c9-e446-4cd0-a003-2972c2cc301f"
   },
   "outputs": [],
   "source": [
    "data_ev = pd.read_csv(\"evaluation.csv\", parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XS1CvB6__jnd"
   },
   "outputs": [],
   "source": [
    "ids = data[\"ids\"]\n",
    "data_clean = data.drop(data[ids.isin(ids[ids.duplicated()])].index)\n",
    "data_clean = data_clean.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "F--eq3jgBuWI"
   },
   "outputs": [],
   "source": [
    "import emot \n",
    "emot_obj = emot.core.emot() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2jQ_kMF8Xsw"
   },
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LV1MxvTiN5V0"
   },
   "outputs": [],
   "source": [
    "def load_dict_smileys():\n",
    "  return {\n",
    "  \";)\":\"smirk\",\n",
    "  \":-)\": \"smiley\",\n",
    "  \":)\": \"smiley\",\n",
    "  \":d\": \"smiley\",\n",
    "  \"xd\": \"smiley\",\n",
    "  \":')\": \"smiley\",\n",
    "  \":'d\": \"smiley\",\n",
    "  \":3\": \"smiley\",\n",
    "  \":]\": \"smiley\",\n",
    "  \":^)\": \"smiley\",\n",
    "  \":-]\": \"smiley\",\n",
    "  \":-3\": \"smiley\",\n",
    "  \":->\": \"smiley\",\n",
    "  \":))\": \"smiley\",\n",
    "  \"8-)\": \"cool\",\n",
    "  \"8)\": \"cool\",\n",
    "  \"8-d\": \"cool\",\n",
    "  \"b)\": \"cool\",\n",
    "  \"bd\": \"cool\",\n",
    "  \":-}\": \"smiley\",\n",
    "  \":>\": \"smiley\",\n",
    "  \":}\": \"smiley\",\n",
    "  \":o)\": \"smiley\",\n",
    "  \":c)\": \"smiley\",\n",
    "  \"=)\": \"smiley\",\n",
    "  \"=d\": \"smiley\",\n",
    "  \"=]\": \"smiley\",\n",
    "  \":-d\": \"smiley\",\n",
    "  \":o\": \"surprised\",\n",
    "  \"=o\": \"surprised\",\n",
    "  \":0\": \"surprised\",\n",
    "  \":(\": \"sad\",\n",
    "  \":c\": \"sad\",\n",
    "  \"=(\": \"sad\",\n",
    "  \"=c\": \"sad\",\n",
    "  \":-(\": \"sad\",\n",
    "  \":-c\": \"sad\",\n",
    "  \":-<\": \"sad\",\n",
    "  \":<\": \"sad\",\n",
    "  \":[\": \"sad\",\n",
    "  \":[[\": \"sad\",\n",
    "  \":{\": \"sad\",\n",
    "  \":'(\": \"crying\",\n",
    "  \":P\": \"playful\",\n",
    "  \"xp\": \"playful\",\n",
    "  \"=P\": \"playful\",\n",
    "  \"xb\": \"playful\",\n",
    "  \"<3\": \"love\",\n",
    "  \"</3\": \"sad\",\n",
    "  \":/\": \"worried\",\n",
    "  \":-/\": \"worried\"\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mGvvSqhVP1uY"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def removeUnicode(text):\n",
    "    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n",
    "    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', text)       \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'',text)\n",
    "    return text\n",
    "\n",
    "def removeURL(text):\n",
    "    \"\"\" Removes url \"\"\"\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',text)\n",
    "    text = re.sub(r'#([^\\s]+)', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def removeNumbers(text):\n",
    "    \"\"\" Removes integers \"\"\"\n",
    "    text = ''.join([i for i in text if not i.isdigit()])         \n",
    "    return text\n",
    "\n",
    "def replaceMultiExclamationMark(text):\n",
    "    \"\"\" Replaces repetitions of exlamation marks \"\"\"\n",
    "    text = re.sub(r\"(\\!)\\1+\", ' multiExclamation ', text) \n",
    "    return text\n",
    "\n",
    "def replaceMultiQuestionMark(text):\n",
    "    \"\"\" Replaces repetitions of question marks \"\"\"\n",
    "    text = re.sub(r\"(\\?)\\1+\", ' multiQuestion ', text) \n",
    "    return text\n",
    "\n",
    "def replaceMultiStopMark(text):\n",
    "    \"\"\" Replaces repetitions of stop marks \"\"\"\n",
    "    text = re.sub(r\"(\\.)\\1+\", ' multiStop ', text) \n",
    "    return text\n",
    "\n",
    "\n",
    "\"\"\" Creates a dictionary with slangs and their equivalents and replaces them \"\"\"\n",
    "with open('slang.txt') as file:\n",
    "    slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "    for line in file if line.strip())\n",
    "\n",
    "slang_words = sorted(slang_map, key=len, reverse=True) # longest first for regex\n",
    "regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
    "replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)].replace(\" \", \"_\"))\n",
    "\n",
    "\n",
    "\"\"\" Replaces contractions from a string to their equivalents \"\"\"\n",
    "contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\n",
    "def replaceContraction(text):\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "    for (pattern, repl) in patterns:\n",
    "        (text, count) = re.subn(pattern, repl, text)\n",
    "    return text\n",
    "\n",
    "def replaceElongated(word):\n",
    "    \"\"\" Replaces an elongated word with its basic form, unless the word exists in the lexicon \"\"\"\n",
    "\n",
    "    repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    repl = r'\\1\\2\\3'\n",
    "    if wordnet.synsets(word):\n",
    "        return word\n",
    "    repl_word = repeat_regexp.sub(repl, word)\n",
    "    if repl_word != word:      \n",
    "        return replaceElongated(repl_word)\n",
    "    else:       \n",
    "        return repl_word\n",
    "\n",
    "\n",
    "\n",
    "def replace(word, pos=None):\n",
    "    \"\"\" Creates a set of all antonyms for the word and if there is only one antonym, it returns it \"\"\"\n",
    "    antonyms = set()\n",
    "    for syn in wordnet.synsets(word, pos=pos):\n",
    "      for lemma in syn.lemmas():\n",
    "        for antonym in lemma.antonyms():\n",
    "          antonyms.add(antonym.name())\n",
    "    if len(antonyms) == 1:\n",
    "      return antonyms.pop()\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "def replaceNegations(text):\n",
    "    \"\"\" Finds \"not\" and antonym for the next word and if found, replaces not and the next word with the antonym \"\"\"\n",
    "    list_text = text.split()\n",
    "    i, l = 0, len(list_text)\n",
    "    words = []\n",
    "    while i < l:\n",
    "      word = list_text[i]\n",
    "      if word == 'not' and i+1 < l:\n",
    "        ant = replace(list_text[i+1])\n",
    "        if ant:\n",
    "          text = text.replace(list_text[i+1], ant)\n",
    "          words.append(ant)\n",
    "          i += 2\n",
    "          continue\n",
    "      i += 1\n",
    "    if words:\n",
    "      text = text.replace(\"not\", \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def addNotTag(text):\n",
    "    \"\"\" Finds \"not,never,no\" and adds the tag NEG_ to all words that follow until the next punctuation \"\"\"\n",
    "    transformed = re.sub(r'\\b(?:not|never|no)\\b[\\w\\s][\\w]+', \n",
    "        lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1NEG_\\2', match.group(0)), \n",
    "        text,\n",
    "        flags=re.IGNORECASE)\n",
    "    return transformed\n",
    "\n",
    "\n",
    "def replaceElongatedWord(text):\n",
    "\n",
    "  tokens = text.split() \n",
    "  for w in tokens:\n",
    "    final_word = replaceElongated(w)\n",
    "    if len(final_word)>1:\n",
    "        text = text.replace(w, final_word)\n",
    "  return text\n",
    "\n",
    "def detect_emoticons_dict(tweet):\n",
    "  tweet = re.sub(\"(http|ftp|https)://[\\w-]+(\\.[\\w-]+)+([\\w.,@?^=%&amp;:/~+#-]*[\\w@?^=%&amp;/~+#-])?\",\"\", tweet)\n",
    "  tweet = re.sub(r'(\\W)(?=\\1)', '', tweet)\n",
    "  tweet = re.sub('[a-z0-9]+:', '', tweet)\n",
    "  tweet = tweet.lower()\n",
    "  smilies = load_dict_smileys()\n",
    "  for key,val in smilies.items():\n",
    "    if key in tweet or key[::-1] in tweet:\n",
    "      new_tweet = tweet.replace(key, val)\n",
    "      tweet = new_tweet\n",
    "  return tweet\n",
    "\n",
    "def detect_emoticons_library(text):\n",
    "  text = re.sub(\"(http|ftp|https)://[\\w-]+(\\.[\\w-]+)+([\\w.,@?^=%&amp;:/~+#-]*[\\w@?^=%&amp;/~+#-])?\",\"\", text)\n",
    "  text = re.sub(r'(\\W)(?=\\1)', '', text) \n",
    "  text = re.sub('[a-z0-9]+:', '', text)\n",
    "  emot = emot_obj.emoticons(str(text).lower().strip())\n",
    "  if emot['flag']:\n",
    "    for count, face in enumerate(emot['value']):\n",
    "      text = text.lower().replace(face, emot['mean'][count].split()[0].lower())\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "z3GQwyVtmeRA"
   },
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "  text = replaceMultiExclamationMark(text)\n",
    "  text = replaceMultiQuestionMark(text)\n",
    "  text = replaceMultiStopMark(text)\n",
    "  text = text.replace(\"&quot|&amp|&lt|&gt|&nbsp|&apos|&cent|&pound|&yen|&euro|&copy|&reg\", \"\")\n",
    "  text = re.sub(\"(a*ha+h[ha]*)\",\"ahah \",text)\n",
    "  text = re.sub(\"(o?l+o+l+[ol]*)\",\"lol \",text) \n",
    "  text = removeUnicode(text) \n",
    "  text = removeURL(text)\n",
    "  text = detect_emoticons_library(text)\n",
    "  text = detect_emoticons_dict(text)\n",
    "  text = replaceElongatedWord(text)\n",
    "  text = replaceSlang(text)\n",
    "  text = replaceContraction(text)\n",
    "  text = removeNumbers(text)\n",
    "  text = replaceNegations(text)\n",
    "  text = \"\".join([char if char not in st.punctuation.replace(\"_\",\"\") else \" \" for char in text])\n",
    "  text = addNotTag(text)\n",
    "  text = re.sub(\"[^A-Za-z0–9_. ]+\",\"\",text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "y5cGwdOom_ti"
   },
   "outputs": [],
   "source": [
    "data_clean[\"text_cleaned\"] = data_clean[\"text\"].apply(lambda x:  preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "En4dEUiCyIWt"
   },
   "outputs": [],
   "source": [
    "data_ev[\"text_cleaned\"] = data_ev[\"text\"].apply(lambda x:  preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "auPDBDRa-q8L"
   },
   "outputs": [],
   "source": [
    "data_clean = data_clean.drop_duplicates(subset=['text_cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2b9Wv4uWELR"
   },
   "source": [
    "# Using TF-IDF model for most similar sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "lMIPNU5-qK50"
   },
   "outputs": [],
   "source": [
    "tv = feature_extraction.text.TfidfVectorizer(max_features=70000, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "62TStd41gAwA"
   },
   "outputs": [],
   "source": [
    "tv_df = tv.fit_transform(data_clean[\"text_cleaned\"][:20000].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ED_QtR4nwyQ8"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_sim = cosine_similarity(tv_df, tv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Cz0zgIjPxoRX"
   },
   "outputs": [],
   "source": [
    "indices = pd.Series(range(20000), index=data_clean[\"text_cleaned\"][:20000])\n",
    "\n",
    "def get_similarity(title, cosine_sim, indices):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "    # Get the pairwsie similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    text_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return data_clean[['text_cleaned', 'sentiment']].iloc[text_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "3RBsr8ctzsXh",
    "outputId": "3f2e328c-1bc1-422e-a760-7a67722d1bdb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>urgh  its raining bad</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5488</th>\n",
       "      <td>is kind_of bored and extremely tired</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>bye tweeters multistop its already  pm here  a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4880</th>\n",
       "      <td>its raining again  what happened to mr  sun</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>i am bored  and tired  i can go home</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17207</th>\n",
       "      <td>bored again</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5614</th>\n",
       "      <td>man i wanted to go outside today but now its r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15048</th>\n",
       "      <td>raining raining raining  i love rain</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16349</th>\n",
       "      <td>its raining multistop best thing to do is stay...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10989</th>\n",
       "      <td>its official its raining in essex no NEG_bq to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            text_cleaned  sentiment\n",
       "793                               urgh  its raining bad           0\n",
       "5488               is kind_of bored and extremely tired           0\n",
       "2499   bye tweeters multistop its already  pm here  a...          1\n",
       "4880      its raining again  what happened to mr  sun             0\n",
       "1959              i am bored  and tired  i can go home            0\n",
       "17207                                       bored again           0\n",
       "5614   man i wanted to go outside today but now its r...          0\n",
       "15048              raining raining raining  i love rain           1\n",
       "16349  its raining multistop best thing to do is stay...          1\n",
       "10989  its official its raining in essex no NEG_bq to...          0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similarity(\"bored and its raining \", cosine_sim, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZSJOFsHiJMW"
   },
   "source": [
    "# Model Performance Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "QmVmAtYoh7ep"
   },
   "outputs": [],
   "source": [
    "####Evaluation metrics\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc \n",
    "\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \n",
    "    print('Accuracy:', np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        4))\n",
    "    print('Precision:', np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "    print('Recall:', np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "    print('F1 Score:', np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "                        \n",
    "\n",
    "def train_predict_model(classifier, \n",
    "                        train_features, train_labels, \n",
    "                        test_features, test_labels):\n",
    "    # build model    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features) \n",
    "    return predictions    \n",
    "\n",
    "\n",
    "def display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n",
    "    \n",
    "    total_classes = len(classes)\n",
    "    level_labels = [total_classes*[0], list(range(total_classes))]\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n",
    "                                  labels=classes)\n",
    "    cm_frame = pd.DataFrame(data=cm, \n",
    "                            columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n",
    "                                                  codes=level_labels), \n",
    "                            index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
    "                                                codes=level_labels)) \n",
    "    print(cm_frame) \n",
    "    \n",
    "def display_classification_report(true_labels, predicted_labels, classes=[1,0]):\n",
    "\n",
    "    report = metrics.classification_report(y_true=true_labels, \n",
    "                                           y_pred=predicted_labels, \n",
    "                                           labels=classes) \n",
    "    print(report)\n",
    "    \n",
    "    \n",
    "    \n",
    "def display_model_performance_metrics(true_labels, predicted_labels, classes=[1,0]):\n",
    "    print('Model Performance metrics:')\n",
    "    print('-'*30)\n",
    "    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n",
    "    print('\\nModel Classification report:')\n",
    "    print('-'*30)\n",
    "    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, \n",
    "                                  classes=classes)\n",
    "    print('\\nPrediction Confusion Matrix:')\n",
    "    print('-'*30)\n",
    "    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n",
    "                             classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBKopF2MmlE2"
   },
   "source": [
    "# Model Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Hj7xF-NcGN8E"
   },
   "outputs": [],
   "source": [
    "tv_train_features = tv.fit_transform(data_clean[\"text_cleaned\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "vpQhn7jYmPWT"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tv_train_features, data_clean[\"sentiment\"].values, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zJdeoQUmix3"
   },
   "source": [
    "**LinearSVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "X-EdPp0wmiNC"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvm = LinearSVC(random_state= 42)\n",
    "\n",
    "lsvm.fit(X_train, y_train)\n",
    "y_pred = lsvm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xOlhPVaWvfNr",
    "outputId": "26b96602-8b96-43ff-8af4-8ffbdf7e39b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7953\n",
      "Precision: 0.7944\n",
      "Recall: 0.7953\n",
      "F1 Score: 0.7944\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75     18696\n",
      "           1       0.81      0.84      0.83     25762\n",
      "\n",
      "    accuracy                           0.80     44458\n",
      "   macro avg       0.79      0.79      0.79     44458\n",
      "weighted avg       0.79      0.80      0.79     44458\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "          Predicted:       \n",
      "                   0      1\n",
      "Actual: 0      13658   5038\n",
      "        1       4064  21698\n"
     ]
    }
   ],
   "source": [
    "display_model_performance_metrics(true_labels=y_test, predicted_labels=y_pred, classes=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LTgv6BG4UUa",
    "outputId": "ddf5a15d-7012-470d-d80d-65bec1e30178"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7883453069616426"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jh2A0UFYVdwX"
   },
   "source": [
    "**RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "wjbfSIBgmoqw"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_jobs=-1, n_estimators= 500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ouYS6RrVmqQs"
   },
   "outputs": [],
   "source": [
    "y_pred = rfc.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "jO-XJeguvi7m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7817\n",
      "Precision: 0.7807\n",
      "Recall: 0.7817\n",
      "F1 Score: 0.7797\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.69      0.73     18696\n",
      "           1       0.79      0.85      0.82     25762\n",
      "\n",
      "    accuracy                           0.78     44458\n",
      "   macro avg       0.78      0.77      0.77     44458\n",
      "weighted avg       0.78      0.78      0.78     44458\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "          Predicted:       \n",
      "                   0      1\n",
      "Actual: 0      12871   5825\n",
      "        1       3881  21881\n"
     ]
    }
   ],
   "source": [
    "display_model_performance_metrics(true_labels=y_test, predicted_labels=y_pred, classes=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "h6B7RkFPUyw9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7723306686331299"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQ6xju6pmw4K"
   },
   "source": [
    "**LogisticRegression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "BLTWVazMmujG"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "cuKdLerJmuNU"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "5ReyLZPP2giu"
   },
   "outputs": [],
   "source": [
    "y_pred = lr.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "lFcCeg_ovjuk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8056\n",
      "Precision: 0.8049\n",
      "Recall: 0.8056\n",
      "F1 Score: 0.8041\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.73      0.76     18696\n",
      "           1       0.81      0.86      0.84     25762\n",
      "\n",
      "    accuracy                           0.81     44458\n",
      "   macro avg       0.80      0.79      0.80     44458\n",
      "weighted avg       0.80      0.81      0.80     44458\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "          Predicted:       \n",
      "                   0      1\n",
      "Actual: 0      13557   5139\n",
      "        1       3505  22257\n"
     ]
    }
   ],
   "source": [
    "display_model_performance_metrics(true_labels=y_test, predicted_labels=y_pred, classes=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "uuyr2YLd7NLV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7978271530069108"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgoaZqR4dFlp"
   },
   "source": [
    "# Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "TYSCH5Sm8XnJ"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_clean[\"text_cleaned\"].values, data_clean[\"sentiment\"].values, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "_K9-eo00Upg-"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=70000, ngram_range=(1,2))\n",
    "\n",
    "porter = PorterStemmer()\n",
    "liemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "def tokenizer_lem(text):\n",
    "    return [liemmatizer.lemmatize(word) for word in text.split()]\n",
    "\n",
    "\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJskowYhU6ue"
   },
   "source": [
    "**Grid for LogisticRegression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrQnWLVRmJBn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid = { 'vect__ngram_range': [(1, 2), (1, 3)],\n",
    "               'vect__tokenizer': [tokenizer_porter, tokenizer_lem, None],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]}\n",
    "              \n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0, max_iter=500))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='f1',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)\n",
    "gs_lr_tfidf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-Jn6BA8VL-O"
   },
   "outputs": [],
   "source": [
    "print('Best parameter set: ' + str(gs_lr_tfidf.best_params_))\n",
    "print('Best f1: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5hHAFTCVN5Q"
   },
   "outputs": [],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('f1 in test: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6c41O2DVnrW"
   },
   "source": [
    "# Model without date and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hps5E6IWtyMv"
   },
   "outputs": [],
   "source": [
    "tv_train_features = tv.fit_transform(data_clean[\"text_cleaned\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wD4IWRDAtyMw"
   },
   "outputs": [],
   "source": [
    "tv_features_ev = tv.transform(data_ev[\"text_cleaned\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHO_UP7HwBTd"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2', max_iter=500, C=1,  solver = 'newton-cg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jWLWxTr6ElZT",
    "outputId": "5cbb1dfe-6eb8-4114-f873-ae17d13cd04a"
   },
   "outputs": [],
   "source": [
    "lr.fit(tv_train_features, data_clean[\"sentiment\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xN71ncoeE8Pn"
   },
   "outputs": [],
   "source": [
    "y_pred_lr = lr.predict(tv_features_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Itmaz6BKFLR9"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "header = 'Id Predicted'\n",
    "header = header.split()\n",
    "\n",
    "file = open('pred.csv', 'w', newline='')\n",
    "\n",
    "with file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "for i in range(len(y_pred_lr)):\n",
    "  pred = y_pred_lr[i]\n",
    "  to_append = f'{i} {pred}'\n",
    "  file = open('pred.csv', 'a', newline='')\n",
    "  with file:\n",
    "      writer = csv.writer(file)\n",
    "      writer.writerow(to_append.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAkKOiZzkMpH"
   },
   "source": [
    "# Improvement using date and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeObcUkUwDuC"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "data_clean['date'] = pd.to_datetime(data_clean['date'])\n",
    "data_clean['date']= data_clean['date'].map(dt.datetime.toordinal)\n",
    "\n",
    "data_ev['date'] = pd.to_datetime(data_ev['date'])\n",
    "data_ev['date']= data_ev['date'].map(dt.datetime.toordinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNUP44cWhG-Q"
   },
   "source": [
    "## Training-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22YA3nsYa_kV"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_clean, data_clean[\"sentiment\"].values, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhALLe8Qlw9K"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OG3Nulc3eHAN"
   },
   "outputs": [],
   "source": [
    "user = enc.fit_transform(X_train['user'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRV06la_eHAN"
   },
   "outputs": [],
   "source": [
    "user_ev = enc.transform(X_test['user'].values.reshape(-1,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJx3lu5UdShp"
   },
   "outputs": [],
   "source": [
    "tv_train_features = tv.fit_transform(X_train[\"text_cleaned\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNUpYI6FdShq"
   },
   "outputs": [],
   "source": [
    "tv_train_features_ev = tv.transform(X_test[\"text_cleaned\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdKWaSY7d2qg"
   },
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "X = scipy.sparse.hstack([tv_train_features, user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQ8bLdj9d2qh"
   },
   "outputs": [],
   "source": [
    "X_ev = scipy.sparse.hstack([tv_train_features_ev, user_ev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-hSF00wid2qh",
    "outputId": "087c7cd7-0cb2-4bb2-ff88-0fba8ec26d37"
   },
   "outputs": [],
   "source": [
    "lr.fit(X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sCsM4I0d2qh"
   },
   "outputs": [],
   "source": [
    "y_pred_lr = lr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRxwPLCnd2qh"
   },
   "outputs": [],
   "source": [
    "y_pred_lr_ev = lr.predict(X_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnvTTv25d2qh"
   },
   "outputs": [],
   "source": [
    "X_test[\"pred_lr\"] = y_pred_lr_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QuILxeTd2qh"
   },
   "outputs": [],
   "source": [
    "X_train[\"pred_lr\"] = y_pred_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imqQlqU4d2qi"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_jobs=-1, n_estimators= 500, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zfV3SDmxd2qi",
    "outputId": "916ab292-c018-4f75-8df9-ed0b39debe19"
   },
   "outputs": [],
   "source": [
    "rfc.fit(X_train[['pred_lr', 'date']].values, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eeg-B7Sed2qi"
   },
   "outputs": [],
   "source": [
    "y_pred_rf_ev = rfc.predict(X_test[['pred_lr', 'date']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3z1hQurIfw0M",
    "outputId": "2678ad67-766d-4d3e-9585-3c8b72c715e5"
   },
   "outputs": [],
   "source": [
    "display_model_performance_metrics(true_labels=y_test, predicted_labels=y_pred_rf_ev, classes=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc1zMlvqmlIq"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hhBlC0Emh1k"
   },
   "outputs": [],
   "source": [
    "tv = feature_extraction.text.TfidfVectorizer(max_features=70000, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLc94QqBmh1k"
   },
   "outputs": [],
   "source": [
    "tv_train_features = tv.fit_transform(data_clean[\"text_cleaned\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCyyB6oRmh1l"
   },
   "outputs": [],
   "source": [
    "tv_train_features_ev = tv.transform(data_ev[\"text_cleaned\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ri5OyvMm6UP"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-nK34Uvm6UP"
   },
   "outputs": [],
   "source": [
    "user = enc.fit_transform(data_clean['user'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyITVcR3m6UQ"
   },
   "outputs": [],
   "source": [
    "user_ev = enc.transform(data_ev['user'].values.reshape(-1,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5ETM7ZHABAa"
   },
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "X = scipy.sparse.hstack([tv_train_features, user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5S7nZZQcUCD"
   },
   "outputs": [],
   "source": [
    "X_ev = scipy.sparse.hstack([tv_train_features_ev, user_ev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJQSxKX_wBTe",
    "outputId": "54c62ad5-ea34-4fd0-e3f7-f6d15f2c94f0"
   },
   "outputs": [],
   "source": [
    "lr.fit(X, data_clean[\"sentiment\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dI2pb_xc2Tm"
   },
   "outputs": [],
   "source": [
    "y_pred_lr = lr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UrEDnWIHwBTe"
   },
   "outputs": [],
   "source": [
    "y_pred_lr_ev = lr.predict(X_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzDsV0cHyCD9"
   },
   "outputs": [],
   "source": [
    "data_ev[\"pred_lr\"] = y_pred_lr_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFyjD0Bmc7CZ"
   },
   "outputs": [],
   "source": [
    "data_clean[\"pred_lr\"] = y_pred_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r13DWPn3xvET"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_jobs=-1, n_estimators= 500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9C6X6FrwyYWl",
    "outputId": "b2237a52-4e7b-431e-b340-8bb4ab8d8cb5"
   },
   "outputs": [],
   "source": [
    "rfc.fit(data_clean[['pred_lr', 'date']].values,  data_clean[\"sentiment\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZgl0SODyYWl"
   },
   "outputs": [],
   "source": [
    "y_pred_rf_ev = rfc.predict(data_ev[['pred_lr', 'date']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUhZGqkJdh7u"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "header = 'Id Predicted'\n",
    "header = header.split()\n",
    "\n",
    "file = open('pred_cascade.csv', 'w', newline='')\n",
    "\n",
    "with file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "for i in range(len(y_pred_rf_ev)):\n",
    "  pred = y_pred_rf_ev[i]\n",
    "  to_append = f'{i} {pred}'\n",
    "  file = open('pred_cascade.csv', 'a', newline='')\n",
    "  with file:\n",
    "      writer = csv.writer(file)\n",
    "      writer.writerow(to_append.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfE2dnDwXMkE"
   },
   "source": [
    "# Evaluation no cascade approach with date and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0BEjHrtXdo2"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUNiaPZfh_Mg"
   },
   "outputs": [],
   "source": [
    "tv = feature_extraction.text.TfidfVectorizer(max_features=70000, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQO093Oeh82q"
   },
   "outputs": [],
   "source": [
    "tv_train_features = tv.fit_transform(data_clean[\"text_cleaned\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vj0xlArjh6AB"
   },
   "outputs": [],
   "source": [
    "tv_train_features_ev = tv.transform(data_ev[\"text_cleaned\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJy31vUZXdo3"
   },
   "outputs": [],
   "source": [
    "user = enc.fit_transform(data_clean['user'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MydryPqbXdo3"
   },
   "outputs": [],
   "source": [
    "user_ev = enc.transform(data_ev['user'].values.reshape(-1,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06pRpZ58Xdo3"
   },
   "outputs": [],
   "source": [
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJrDhoiSXrwm"
   },
   "outputs": [],
   "source": [
    "X = scipy.sparse.hstack([tv_train_features, user, data_clean['date'].values.reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-q5kGIv0XuIj"
   },
   "outputs": [],
   "source": [
    "X_ev = scipy.sparse.hstack([tv_train_features_ev, user_ev, data_ev['date'].values.reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGF6ISCTX8p_"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_jobs=-1, n_estimators= 100, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "twRaIqJVYEEV",
    "outputId": "2c49d90f-c2dc-4045-fc97-03812daca5dd"
   },
   "outputs": [],
   "source": [
    "rfc.fit(X,  data_clean[\"sentiment\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crf9qNd_YKH8"
   },
   "outputs": [],
   "source": [
    "y_pred_rf_ev = rfc.predict(X_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pKCe0xPubOg"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "header = 'Id Predicted'\n",
    "header = header.split()\n",
    "\n",
    "file = open('pred_randomForest.csv', 'w', newline='')\n",
    "\n",
    "with file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "for i in range(len(y_pred_rf_ev)):\n",
    "  pred = y_pred_rf_ev[i]\n",
    "  to_append = f'{i} {pred}'\n",
    "  file = open('pred_randomForest.csv', 'a', newline='')\n",
    "  with file:\n",
    "      writer = csv.writer(file)\n",
    "      writer.writerow(to_append.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9WfMoFPxStw"
   },
   "outputs": [],
   "source": [
    "from imblearn.metrics import classification_report_imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nuBnxCALxJEk",
    "outputId": "611f64f2-3fa6-4672-f09f-4b96a3932df1"
   },
   "outputs": [],
   "source": [
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "zZSJOFsHiJMW",
    "KfE2dnDwXMkE",
    "d8yNDLLhxpN6"
   ],
   "name": "ex_def.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
